{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFeURWkJrh1c",
        "outputId": "94a9f72a-51dd-4b23-80ba-7343f9045587"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensorflow 2.8.0\n",
            "nfp 0.3.8+0.ge19d476.dirty\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "import nfp\n",
        "\n",
        "print(f\"tensorflow {tf.__version__}\")\n",
        "print(f\"nfp {nfp.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjtceBFMrh1f",
        "outputId": "d67f3517-8589-4cc5-afdd-3e36d2c3b194"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Species</th>\n",
              "      <th>CAS</th>\n",
              "      <th>Ref</th>\n",
              "      <th>Type</th>\n",
              "      <th>YSI</th>\n",
              "      <th>YSI_err</th>\n",
              "      <th>SMILES</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1,2-diphenylbenzene</td>\n",
              "      <td>84-15-1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>aromatic</td>\n",
              "      <td>1338.9</td>\n",
              "      <td>50.0</td>\n",
              "      <td>c1ccc(-c2ccccc2-c2ccccc2)cc1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cyclopenta[def]phenanthrene</td>\n",
              "      <td>203-64-5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>aromatic</td>\n",
              "      <td>1312.8</td>\n",
              "      <td>73.5</td>\n",
              "      <td>c1cc2c3c(c1)ccc1cccc(c13)C2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fluoranthene</td>\n",
              "      <td>206-44-0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>aromatic</td>\n",
              "      <td>1291.9</td>\n",
              "      <td>72.4</td>\n",
              "      <td>c1ccc2c(c1)-c1cccc3cccc-2c13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1,3-diphenylbenzene</td>\n",
              "      <td>92-06-8</td>\n",
              "      <td>2.0</td>\n",
              "      <td>aromatic</td>\n",
              "      <td>1286.6</td>\n",
              "      <td>72.3</td>\n",
              "      <td>c1ccc(-c2cccc(-c3ccccc3)c2)cc1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pyrene</td>\n",
              "      <td>129-00-0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>aromatic</td>\n",
              "      <td>1250.1</td>\n",
              "      <td>70.1</td>\n",
              "      <td>c1cc2ccc3cccc4ccc(c1)c2c34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Species       CAS  Ref      Type     YSI  YSI_err  \\\n",
              "0          1,2-diphenylbenzene   84-15-1  2.0  aromatic  1338.9     50.0   \n",
              "1  cyclopenta[def]phenanthrene  203-64-5  2.0  aromatic  1312.8     73.5   \n",
              "2                 fluoranthene  206-44-0  2.0  aromatic  1291.9     72.4   \n",
              "3          1,3-diphenylbenzene   92-06-8  2.0  aromatic  1286.6     72.3   \n",
              "4                       pyrene  129-00-0  2.0  aromatic  1250.1     70.1   \n",
              "\n",
              "                           SMILES  \n",
              "0    c1ccc(-c2ccccc2-c2ccccc2)cc1  \n",
              "1     c1cc2c3c(c1)ccc1cccc(c13)C2  \n",
              "2    c1ccc2c(c1)-c1cccc3cccc-2c13  \n",
              "3  c1ccc(-c2cccc(-c3ccccc3)c2)cc1  \n",
              "4      c1cc2ccc3cccc4ccc(c1)c2c34  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the input data, here YSI (10.1016/j.combustflame.2017.12.005)\n",
        "# ysi.csv available from https://github.com/rajuram99/BDE-ESTIMATOR/blob/main/ysi.csv\n",
        "ysi = pd.read_csv('../data/ysi.csv')\n",
        "ysi.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdwTwYGHrh1g",
        "outputId": "f3b93f4a-1a5b-4128-a0f6-00d9c2a9ffde"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(341, 50, 50)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Split the data into training, validation, and test sets\n",
        "valid, test, train = np.split(ysi.SMILES.sample(frac=1., random_state=1), [50, 100])\n",
        "len(train), len(valid), len(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP-ryPlGrh1h"
      },
      "outputs": [],
      "source": [
        "# Define how to featurize the input molecules\n",
        "from nfp.preprocessing.mol_preprocessor import SmilesPreprocessor\n",
        "from nfp.preprocessing.features import get_ring_size\n",
        "\n",
        "\n",
        "def atom_featurizer(atom):\n",
        "    \"\"\" Return an string representing the atom type\n",
        "    \"\"\"\n",
        "\n",
        "    return str((\n",
        "        atom.GetSymbol(),\n",
        "        atom.GetIsAromatic(),\n",
        "        get_ring_size(atom, max_size=6),\n",
        "        atom.GetDegree(),\n",
        "        atom.GetTotalNumHs(includeNeighbors=True)\n",
        "    ))\n",
        "\n",
        "\n",
        "def bond_featurizer(bond, flipped=False):\n",
        "    \"\"\" Get a similar classification of the bond type.\n",
        "    Flipped indicates which 'direction' the bond edge is pointing. \"\"\"\n",
        "    \n",
        "    if not flipped:\n",
        "        atoms = \"{}-{}\".format(\n",
        "            *tuple((bond.GetBeginAtom().GetSymbol(),\n",
        "                    bond.GetEndAtom().GetSymbol())))\n",
        "    else:\n",
        "        atoms = \"{}-{}\".format(\n",
        "            *tuple((bond.GetEndAtom().GetSymbol(),\n",
        "                    bond.GetBeginAtom().GetSymbol())))\n",
        "    \n",
        "    btype = str(bond.GetBondType())\n",
        "    ring = 'R{}'.format(get_ring_size(bond, max_size=6)) if bond.IsInRing() else ''\n",
        "    \n",
        "    return \" \".join([atoms, btype, ring]).strip()\n",
        "\n",
        "\n",
        "preprocessor = SmilesPreprocessor(atom_features=atom_featurizer, bond_features=bond_featurizer,\n",
        "                                  explicit_hs=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1gwmR8Qrh1i",
        "outputId": "9aeedd7c-db66-4a32-9aba-9928c919a4f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "before pre-allocating\n",
            "{'unk': 1}\n",
            "\n",
            "after pre-allocating\n",
            "{'unk': 1, \"('C', False, 0, 1, 3)\": 2, \"('C', False, 0, 2, 2)\": 3, \"('C', False, 0, 3, 0)\": 4, \"('O', False, 0, 1, 0)\": 5, \"('O', False, 0, 2, 0)\": 6, \"('C', False, 0, 1, 2)\": 7, \"('C', False, 0, 2, 1)\": 8, \"('C', False, 0, 3, 1)\": 9, \"('C', False, 0, 4, 0)\": 10, \"('C', True, 'max', 2, 1)\": 11, \"('C', True, 'max', 3, 0)\": 12, \"('C', False, 'max', 2, 2)\": 13, \"('O', False, 0, 1, 1)\": 14, \"('C', False, 0, 1, 1)\": 15, \"('C', False, 0, 2, 0)\": 16, \"('C', False, 'max', 3, 1)\": 17, \"('N', False, 0, 1, 0)\": 18, \"('Br', False, 0, 1, 0)\": 19, \"('C', False, 'max', 2, 1)\": 20, \"('C', True, 5, 3, 0)\": 21, \"('C', False, 5, 2, 2)\": 22, \"('C', False, 5, 2, 1)\": 23, \"('C', False, 5, 3, 1)\": 24, \"('C', False, 5, 3, 0)\": 25, \"('C', False, 'max', 3, 0)\": 26, \"('O', False, 5, 2, 0)\": 27, \"('O', False, 'max', 2, 0)\": 28, \"('F', False, 0, 1, 0)\": 29, \"('C', True, 5, 2, 1)\": 30, \"('C', False, 'max', 4, 0)\": 31, \"('Cl', False, 0, 1, 0)\": 32, \"('C', False, 3, 3, 1)\": 33, \"('C', False, 3, 2, 2)\": 34, \"('O', False, 3, 2, 0)\": 35}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-02-28 15:43:02.624213: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# Initially, the preprocessor has no data on atom types, so we have to loop over the \n",
        "# training set once to pre-allocate these mappings\n",
        "print(\"before pre-allocating\")\n",
        "print(preprocessor.atom_tokenizer._data)\n",
        "\n",
        "for smiles in train:\n",
        "    preprocessor(smiles, train=True)\n",
        "    \n",
        "print()\n",
        "print(\"after pre-allocating\")\n",
        "print(preprocessor.atom_tokenizer._data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CT1B6fwRrh1j",
        "outputId": "6ba442dd-84f6-4dbf-c17f-7243e45b3e53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 2,  3, 14], dtype=int32)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Main input types for a SMILES-based prediction\n",
        "smiles = 'CCO'\n",
        "\n",
        "# Atom types, as integer classes\n",
        "preprocessor(smiles, train=True)['atom']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5Zz7R8erh1k",
        "outputId": "4c7591b5-6d79-40ee-f837-305d58a8314b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2, 2, 5, 6], dtype=int32)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Bond types, as integer classes\n",
        "preprocessor(smiles, train=True)['bond']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYEuKfe0rh1l",
        "outputId": "641c0e56-72dd-4017-852c-a68c21939bbd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 1],\n",
              "       [1, 0],\n",
              "       [1, 2],\n",
              "       [2, 1]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# A connectivity array, where row i indicates bond i connects atom j to atom k\n",
        "preprocessor(smiles, train=True)['connectivity']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egQxwCR-rh1m"
      },
      "outputs": [],
      "source": [
        "# Construct the tf.data pipeline. There's a lot of specifying data types and\n",
        "# expected shapes for tensorflow to pre-allocate the necessary arrays. But \n",
        "# essentially, this is responsible for calling the input constructor, batching \n",
        "# together multiple molecules, and padding the resulting molecules so that all\n",
        "# molecules in the same batch have the same number of atoms (we pad with zeros,\n",
        "# hence why the atom and bond types above start with 1 as the unknown class)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: ((preprocessor(row.SMILES, train=False), row.YSI)\n",
        "             for i, row in ysi[ysi.SMILES.isin(train)].iterrows()),\n",
        "    output_signature=(preprocessor.output_signature, tf.TensorSpec((), dtype=tf.float32)))\\\n",
        "    .cache().shuffle(buffer_size=200)\\\n",
        "    .padded_batch(batch_size=64)\\\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: ((preprocessor(row.SMILES, train=False), row.YSI)\n",
        "             for i, row in ysi[ysi.SMILES.isin(valid)].iterrows()),\n",
        "    output_signature=(preprocessor.output_signature, tf.TensorSpec((), dtype=tf.float32)))\\\n",
        "    .cache()\\\n",
        "    .padded_batch(batch_size=64)\\\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1BfhuEgrh1o",
        "outputId": "1f0c1d11-5855-4856-aee6-143f74d06bdf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-02-28 15:43:03.219907: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[ 2,  4,  5, ...,  0,  0,  0],\n",
              "       [ 2,  3,  3, ...,  0,  0,  0],\n",
              "       [ 2,  3,  8, ...,  0,  0,  0],\n",
              "       ...,\n",
              "       [ 2,  4,  2, ...,  0,  0,  0],\n",
              "       [11, 11, 21, ...,  0,  0,  0],\n",
              "       [ 7,  4,  2, ...,  0,  0,  0]], dtype=int32)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs, outputs = next(train_dataset.as_numpy_iterator())\n",
        "inputs['atom']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CkuyMiRrh1o"
      },
      "outputs": [],
      "source": [
        "## Define the keras model\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Input layers\n",
        "atom = layers.Input(shape=[None], dtype=tf.int64, name='atom')\n",
        "bond = layers.Input(shape=[None], dtype=tf.int64, name='bond')\n",
        "connectivity = layers.Input(shape=[None, 2], dtype=tf.int64, name='connectivity')\n",
        "\n",
        "num_features = 8  # Controls the size of the model\n",
        "\n",
        "# Convert from a single integer defining the atom state to a vector\n",
        "# of weights associated with that class\n",
        "atom_state = layers.Embedding(preprocessor.atom_classes, num_features,\n",
        "                              name='atom_embedding', mask_zero=True)(atom)\n",
        "\n",
        "# Ditto with the bond state\n",
        "bond_state = layers.Embedding(preprocessor.bond_classes, num_features,\n",
        "                              name='bond_embedding', mask_zero=True)(bond)\n",
        "\n",
        "# Here we use our first nfp layer. This is an attention layer that looks at\n",
        "# the atom and bond states and reduces them to a single, graph-level vector. \n",
        "# mum_heads * units has to be the same dimension as the atom / bond dimension\n",
        "global_state = nfp.GlobalUpdate(units=8, num_heads=1)([atom_state, bond_state, connectivity])\n",
        "\n",
        "for _ in range(3):  # Do the message passing\n",
        "    new_bond_state = nfp.EdgeUpdate()([atom_state, bond_state, connectivity, global_state])\n",
        "    bond_state = layers.Add()([bond_state, new_bond_state])\n",
        "    \n",
        "    new_atom_state = nfp.NodeUpdate()([atom_state, bond_state, connectivity, global_state])\n",
        "    atom_state = layers.Add()([atom_state, new_atom_state])\n",
        "    \n",
        "    new_global_state = nfp.GlobalUpdate(units=8, num_heads=1)(\n",
        "        [atom_state, bond_state, connectivity, global_state]) \n",
        "    global_state = layers.Add()([global_state, new_global_state])\n",
        "\n",
        "    \n",
        "# Since the final prediction is a single, molecule-level property (YSI), we \n",
        "# reduce the last global state to a single prediction.\n",
        "ysi_prediction = layers.Dense(1)(global_state)\n",
        "\n",
        "# Construct the tf.keras model\n",
        "model = tf.keras.Model([atom, bond, connectivity], [ysi_prediction])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mafpBwdQrh1p",
        "outputId": "d368913d-38dd-4789-cf5b-5d3bf58286a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "6/6 [==============================] - 7s 343ms/step - loss: 192.5133 - val_loss: 156.2761\n",
            "Epoch 2/25\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 191.7135 - val_loss: 155.0828\n",
            "Epoch 3/25\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 190.0994 - val_loss: 152.4968\n",
            "Epoch 4/25\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 186.4315 - val_loss: 146.0459\n",
            "Epoch 5/25\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 176.6360 - val_loss: 127.8532\n",
            "Epoch 6/25\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 162.1535 - val_loss: 122.6636\n",
            "Epoch 7/25\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 155.8913 - val_loss: 116.2806\n",
            "Epoch 8/25\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 149.0768 - val_loss: 112.1541\n",
            "Epoch 9/25\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 143.1971 - val_loss: 109.4144\n",
            "Epoch 10/25\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 127.7537 - val_loss: 98.0665\n",
            "Epoch 11/25\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 105.5100 - val_loss: 80.6688\n",
            "Epoch 12/25\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 73.2531 - val_loss: 69.9882\n",
            "Epoch 13/25\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 66.7058 - val_loss: 67.9223\n",
            "Epoch 14/25\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 66.4261 - val_loss: 62.6712\n",
            "Epoch 15/25\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 61.7337 - val_loss: 61.1835\n",
            "Epoch 16/25\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 59.7103 - val_loss: 61.5932\n",
            "Epoch 17/25\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 57.2746 - val_loss: 57.8572\n",
            "Epoch 18/25\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 55.1694 - val_loss: 55.1182\n",
            "Epoch 19/25\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 54.0656 - val_loss: 53.0483\n",
            "Epoch 20/25\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 53.0506 - val_loss: 55.2939\n",
            "Epoch 21/25\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 54.0662 - val_loss: 55.1363\n",
            "Epoch 22/25\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 51.7510 - val_loss: 51.9534\n",
            "Epoch 23/25\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 50.5777 - val_loss: 49.6114\n",
            "Epoch 24/25\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 49.7325 - val_loss: 48.9311\n",
            "Epoch 25/25\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 49.1177 - val_loss: 49.2412\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f949219d970>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.compile(loss='mae', optimizer=tf.keras.optimizers.Adam(1E-3))\n",
        "\n",
        "# Fit the model. The first epoch is slower, since it needs to cache\n",
        "# the preprocessed molecule inputs\n",
        "model.fit(train_dataset, validation_data=valid_dataset, epochs=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2baGHxzrh1p"
      },
      "outputs": [],
      "source": [
        "# Here, we create a test dataset that doesn't assume we know the values for the YSI\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: (preprocessor(smiles, train=False)\n",
        "             for smiles in test),\n",
        "    output_signature=preprocessor.output_signature)\\\n",
        "    .padded_batch(batch_size=64)\\\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTK_tWpGrh1p",
        "outputId": "93b79e39-2918-4b77-fd8f-afb580c9a815"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "40.749891374588"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Here are the predictions on the test set\n",
        "test_predictions = model.predict(test_dataset)\n",
        "test_db_values = ysi.set_index('SMILES').reindex(test).YSI.values\n",
        "\n",
        "np.abs(test_db_values - test_predictions.flatten()).mean()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}